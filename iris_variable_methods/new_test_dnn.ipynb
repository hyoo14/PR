{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "import csv\n",
    "\n",
    "fr = open('th4_train.csv', 'r')\n",
    "fr.readline()\n",
    "read_data = csv.reader(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data = list(read_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(mnist_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16877\n"
     ]
    }
   ],
   "source": [
    "print( len(mnist_data) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr.close()\n",
    "\n",
    "train_image = []\n",
    "train_label = []\n",
    "\n",
    "##############from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "##############mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "\n",
    "# hyper parameters\n",
    "learning_rate = 0.1\n",
    "training_epochs = 1\n",
    "batch_size = 900\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_func=tf.train.AdamOptimizer\n",
    "act_func=tf.nn.relu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_batch(batch_size):\n",
    "    \n",
    "    batch_image = []\n",
    "    batch_label = []\n",
    "    random.shuffle(mnist_data)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        each_image = mnist_data[i][1:9]\n",
    "        label1 = mnist_data[i][9]\n",
    "        label2 = mnist_data[i][10]\n",
    "        label3 = mnist_data[i][11]\n",
    "        #for j in range(len(each_image)):\n",
    "        #    each_image[j] = ((float)(each_image[j]) / 255)\n",
    "        \n",
    "        batch_image.append(each_image)\n",
    "        batch_label.append([label1, label2, label3])\n",
    "        \"\"\"if mnist_data[i][0] == '0':\n",
    "            batch_label.append([1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "        \"\"\"\n",
    "    #print(batch_image[0:batch_size], '     ',batch_label[0:batch_size])\n",
    "    return batch_image[0:batch_size], batch_label[0:batch_size] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('input') as scope:\n",
    "    X = tf.placeholder(tf.float32, [None, 8], name = 'image')\n",
    "    y = tf.placeholder(tf.float32, [None, 3], name = 'label')    \n",
    "\n",
    "W = []\n",
    "b = []\n",
    "L = []\n",
    "\n",
    "for i in range(0, 151):\n",
    "    W.append([])\n",
    "    b.append([])\n",
    "    L.append([])\n",
    "\n",
    "with tf.variable_scope('layer1') as scope:\n",
    "    W[1] = tf.get_variable(\"W\", shape = [8, 8], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b[1] = tf.Variable(tf.random_normal([8]))\n",
    "    L[1] = act_func(tf.add(tf.matmul(X, W[1]), b[1]))#########################################ex1-2act_func\n",
    "    \n",
    "    tf.summary.histogram(\"X\", X)\n",
    "    tf.summary.histogram(\"weights\", W[1])\n",
    "    tf.summary.histogram(\"bias\", b[1])\n",
    "    tf.summary.histogram(\"layer\", L[1]) \n",
    "    \n",
    "\n",
    "    \n",
    "for i in range(2, 151):\n",
    "    layer_name = 'layer' + str(i)    \n",
    "    with tf.variable_scope(layer_name) as scope:\n",
    "        W[i] = tf.get_variable(\"W\", shape = [8, 8], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        b[i] = tf.Variable(tf.random_normal([8]))\n",
    "        L[i] = act_func(tf.add(tf.matmul(L[i-1], W[i]), b[i]))#########################################ex1-2act_func\n",
    "        \n",
    "        tf.summary.histogram(\"weights\", W[i])\n",
    "        tf.summary.histogram(\"bias\", b[i])\n",
    "        tf.summary.histogram(\"layer\", L[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wow  150\n",
      "Tensor(\"layer150/Relu:0\", shape=(?, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print('wow ',i)\n",
    "print(L[150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('layer_out') as scope:\n",
    "    W0 = tf.get_variable(\"W\", shape = [8, 3], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b0 = tf.Variable(tf.random_normal([3]))\n",
    "    y_ = tf.add(tf.matmul(L[150], W0), b0)\n",
    "    \n",
    "    tf.summary.histogram(\"weights\", W0)\n",
    "    tf.summary.histogram(\"bias\", b0)\n",
    "    tf.summary.histogram(\"logits\", y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-6bfa705df1c0>:1: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'loss:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = y_, labels = y))\n",
    "optimizer = optimizer_func(learning_rate = learning_rate).minimize(loss) #################################ex1-2optimizer\n",
    "tf.summary.scalar(\"loss\", loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16877\n"
     ]
    }
   ],
   "source": [
    "print(len(mnist_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(name):\n",
    "    fr = open(name,'r')\n",
    "    fr.readline()\n",
    "    read_data = csv.reader(fr)\n",
    "    ret_data = list(read_data)\n",
    "    fr.close()\n",
    "    return ret_data   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forecast\n",
    "fc_dic = []\n",
    "fc_add = read_data('forecast_london_me_20180523.csv')\n",
    "\n",
    "#dictionary에 aq데이터를 입력\n",
    "for i in range(len(fc_add)):    \n",
    "    \n",
    "    if fc_add[i][1] == 'london_grid_430':            \n",
    "        fc_dic.append(fc_add[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 loss =  113.754999\n",
      "[['17.1', '26.9', '32.3', '16.08', '1014.5234', '76.0', '9.27', '12.7']]\n",
      "[[0.3504181, 2.341681, 1.4781897, '12.62', '1017.48411373436', '79.0', '38.32', '14.77']]\n",
      "[[0.3504181, 2.341681, 1.4781897, '12.73', '1017.16813019273', '82.0', '40.98', '14.46']]\n",
      "[[0.3504181, 2.341681, 1.4781897, '12.94', '1016.911171875', '84.0', '44.05', '14.3']]\n",
      "[[0.3504181, 2.341681, 1.4781897, '13.24', '1016.7227993083', '84.0', '47.86', '14.22']]\n",
      "[[0.3504181, 2.341681, 1.4781897, '13.62', '1016.61870012883', '83.0', '51.6', '13.99']]\n",
      "[[0.3504181, 2.341681, 1.4781897, '14.04', '1016.61609375', '82.0', '54.35', '13.28']]\n",
      "[[0.3504181, 2.341681, 1.4781897, '14.54', '1016.70468324538', '82.0', '55.67', '11.92']]\n",
      "[[0.3504181, 2.341681, 1.4781897, '15.32', '1016.76410632901', '82.0', '57.06', '10.58']]\n",
      "[[0.3504181, 2.341681, 1.4781897, '16.64', '1016.646484375', '79.0', '60.85', '10.18']]\n",
      "[[0.3504181, 2.341681, 1.4781897, '18.59', '1016.25643935367', '71.0', '67.16', '11.41']]\n",
      "[[0.3504181, 2.341681, 1.4781897, '20.73', '1015.70859561997', '62.0', '72.62', '13.6']]\n",
      "[[0.3504181, 2.341681, 1.4781897, '22.44', '1015.170078125', '53.0', '76.22', '15.6']]\n",
      "[[0.3504181, 2.341681, 1.4781897, '23.27', '1014.76984001122', '48.0', '78.5', '16.5']]\n",
      "[[0.3504181, 2.341681, 1.4781897, '23.38', '1014.48414718648', '45.0', '79.9', '16.54']]\n",
      "[[0.3504181, 2.341681, 1.4781897, '23.04', '1014.25109375', '45.0', '80.36', '16.23']]\n",
      "[[0.3504181, 2.341681, 1.4781897, '22.5', '1014.03053740702', '46.0', '79.63', '15.95']]\n",
      "[[0.3504181, 2.341681, 1.4781897, '21.79', '1013.86939028689', '50.0', '77.39', '15.47']]\n",
      "[[0.3504181, 2.341681, 1.4781897, '20.86', '1013.836328125', '54.0', '73.07', '14.46']]\n",
      "[[0.3504181, 2.341681, 1.4781897, '19.73', '1013.9710543422', '60.0', '65.44', '12.84']]\n",
      "[[0.3504181, 2.341681, 1.4781897, '18.56', '1014.19738310116', '66.0', '53.85', '11.23']]\n",
      "[[0.3504181, 2.341681, 1.4781897, '17.56', '1014.41015625', '72.0', '39.85', '10.32']]\n",
      "[[0.3504181, 2.341681, 1.4781897, '16.88', '1014.51771628899', '76.0', '27.7', '10.29']]\n",
      "[[0.3504181, 2.341681, 1.4781897, '16.4', '1014.48240832699', '78.0', '18.86', '10.73']]\n",
      "[[0.3504181, 2.341681, 1.4781897, '15.94', '1014.280078125', '80.0', '12.34', '11.29']]\n",
      "[[0.3504181, 2.341681, 1.4781897, '15.38', '1013.91490341851', '83.0', '6.84', '11.82']]\n",
      "[[0.3504181, 2.341681, 1.4781897, '14.8', '1013.50438984089', '85.0', '1.78', '12.22']]\n",
      "[[0.3504181, 2.341681, 1.4781897, '14.34', '1013.194375', '86.0', '356.75', '12.43']]\n",
      "[[0.3504181, 2.341681, 1.4781897, '14.11', '1013.09074584716', '86.0', '351.46', '12.44']]\n",
      "[[0.3504181, 2.341681, 1.4781897, '14.14', '1013.13958670759', '84.0', '346.19', '12.21']]\n",
      "[[0.3504181, 2.341681, 1.4781897, '14.44', '1013.24703125', '82.0', '341.31', '11.69']]\n",
      "[[0.3504181, 2.341681, 1.4781897, '15.01', '1013.33473761416', '79.0', '336.96', '10.82']]\n",
      "[[0.3504181, 2.341681, 1.4781897, '15.82', '1013.38646182412', '76.0', '332.18', '9.67']]\n",
      "[[0.3504181, 2.341681, 1.4781897, '16.84', '1013.401484375', '72.0', '325.38', '8.41']]\n",
      "[[0.3504181, 2.341681, 1.4781897, '17.99', '1013.38164628879', '68.0', '314.97', '7.27']]\n",
      "[[0.3504181, 2.341681, 1.4781897, '19.12', '1013.33903069499', '64.0', '302.01', '6.38']]\n",
      "[[0.3504181, 2.341681, 1.4781897, '20.04', '1013.28828125', '62.0', '289.3', '5.63']]\n",
      "[[0.3504181, 2.341681, 1.4781897, '20.6', '1013.23984910569', '60.0', '278.86', '4.72']]\n",
      "[[0.3504181, 2.341681, 1.4781897, '20.86', '1013.18741539591', '59.0', '269.12', '3.63']]\n",
      "[[0.3504181, 2.341681, 1.4781897, '20.96', '1013.12046875', '59.0', '257.05', '2.53']]\n",
      "[[0.3504181, 2.341681, 1.4781897, '20.96', '1013.04912511252', '58.0', '237.86', '1.65']]\n",
      "[[0.3504181, 2.341681, 1.4781897, '20.8', '1013.06600968895', '57.0', '204.17', '1.09']]\n",
      "[[0.3504181, 2.341681, 1.4781897, '20.41', '1013.284375', '59.0', '156.49', '0.99']]\n",
      "[[0.3504181, 2.341681, 1.4781897, '19.73', '1013.76562093033', '63.0', '118.19', '1.33']]\n",
      "[[0.3504181, 2.341681, 1.4781897, '18.88', '1014.3637368205', '68.0', '95.57', '2.05']]\n",
      "[[0.3504181, 2.341681, 1.4781897, '18.04', '1014.880859375', '74.0', '84.49', '3.22']]\n",
      "[[0.3504181, 2.341681, 1.4781897, '17.34', '1015.16331593466', '78.0', '79.58', '4.84']]\n"
     ]
    }
   ],
   "source": [
    "global_step = 0\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    with tf.device(\"/gpu:0\"):\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "        for epoch in range(training_epochs):\n",
    "            total_batch = int( len(mnist_data) / batch_size )\n",
    "            avg_loss = 0\n",
    "        \n",
    "            for i in range(total_batch):\n",
    "                batch_xs, batch_ys = get_train_batch(batch_size)\n",
    "                feed_dict = {X: batch_xs, y: batch_ys}\n",
    "                s, I, _ = sess.run([summary, loss, optimizer], feed_dict = feed_dict)\n",
    "                #print('s is ',s,'   I is',I)\n",
    "                global_step += 1\n",
    "                avg_loss += I\n",
    "            print('Epoch:', '%02d' % (epoch + 1), 'loss = ', '{:6f}'.format(avg_loss / total_batch))\n",
    "            #print('s is ',s,'   I is',I)\n",
    "    \n",
    "        correct_prediction = tf.equal(tf.argmax(y_, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "        for i in range(len(mnist_data)):    \n",
    "            if mnist_data[i][0] == 'TH4#1297':\n",
    "                #print(len(mnist_data[i]))\n",
    "                pm25 = mnist_data[i][9]\n",
    "                pm10 = mnist_data[i][10]\n",
    "                o3 = mnist_data[i][11]     \n",
    "                test_data = []\n",
    "                test_data.append([pm25, pm10, o3, mnist_data[i][4], mnist_data[i][5], mnist_data[i][6], mnist_data[i][8], mnist_data[i][7] ])\n",
    "                break             \n",
    "                #tf.add(tf.matmul(L[150], W0), b0)\n",
    "        result = []\n",
    "        output = sess.run(tf.add(tf.matmul(L[150], W0), b0), feed_dict = {X: test_data})\n",
    "        pm25 = output[0][0]\n",
    "        pm10 = output[0][1]\n",
    "        o3 = output[0][2]\n",
    "        result.append([pm25, pm10, o3])\n",
    "        \n",
    "        for i in range(47):\n",
    "            print(test_data)\n",
    "            test_data[0][0] = pm25\n",
    "            test_data[0][1] = pm10\n",
    "            test_data[0][2] = o3\n",
    "            \n",
    "            if fc_dic[i][4] != '':\n",
    "                test_data[0][3] = fc_dic[i][4]\n",
    "            if fc_dic[i][5] != '':\n",
    "                test_data[0][4] = fc_dic[i][5]\n",
    "            if fc_dic[i][6] != '':\n",
    "                test_data[0][5] = fc_dic[i][6]\n",
    "            if fc_dic[i][8] != '':\n",
    "                test_data[0][6] = fc_dic[i][8]\n",
    "            if fc_dic[i][7] != '':\n",
    "                test_data[0][7] = fc_dic[i][7]\n",
    "            \n",
    "            output = sess.run(tf.add(tf.matmul(L[150], W0), b0), feed_dict = {X: test_data})\n",
    "            \n",
    "            pm25 = output[0][0]\n",
    "            pm10 = output[0][1]\n",
    "            o3 = output[0][2]\n",
    "            result.append([pm25, pm10, o3])\n",
    "            \n",
    "        \n",
    "        #print(len(output))\n",
    "        #print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4758003\n"
     ]
    }
   ],
   "source": [
    " print(output[0][2])       \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvfile = open('th4TEST', \"w\", newline=\"\")#csvfile = open(\"./submit.csv\", \"w\", newline=\"\")\n",
    "    \n",
    "csvwriter = csv.writer(csvfile)\n",
    "csvwriter.writerow(['test_id','PM2.5','PM10','O3'])\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "for j in range( len( result ) ):\n",
    "    i = 'TH4'\n",
    "    st_t = i + '#' + str(j)\n",
    "    csvwriter.writerow([st_t, result[j][0], result[j][1], result[j][2]])\n",
    "\n",
    "\n",
    "csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
